# LangChain Interviewer

## Overview

This is just a sample project I created for learning how to use LangChain.

It's a Python application built using LangChain that simulates a technical interviewer. It aims to assess candidates by leveraging Large Language Models (LLMs) to process resumes, ask relevant questions based on interview stages, evaluate answers, and manage the overall interview flow. It supports multiple LLM providers like Google and OpenAI.

## Features

* **Multi-Stage Interview Simulation:** Guides the conversation through different stages: Icebreaker, Recent Experience, AI Basics, AI Advanced, Coding Setup, Coding Challenge, and Conclusion.
* **Resume Processing:** Loads candidate resumes from PDF files, extracts text, and uses an LLM to identify the candidate's name and structured work experience.
* **Job Description Loading:** Reads job descriptions from PDF files to potentially tailor the interview. (Note: Structured extraction from JD is commented out in the provided code).
* **Vector Store Integration:** Uses FAISS to create a vector store from the resume (and potentially job description) for retrieving relevant context during the interview.
* **LLM Provider Flexibility:** Supports Google (Gemini) and OpenAI (GPT) models, configurable via a JSON file.
* **Conversation Memory:** Maintains conversation history using `ConversationBufferMemory` to ensure contextually relevant follow-up questions.
* **Dynamic Question Generation:** Generates interview questions dynamically based on the current stage, conversation history, and extracted resume details.
* **Answer Evaluation:** Provides constructive feedback on the candidate's answers using a separate LLM chain.
* **Coding Challenge Module:** Allows setting preferred programming language (Python/Java) and difficulty (easy/medium/hard) for a coding problem generated by the LLM.
* **Configuration File:** Uses `config.json` for easy setup of LLM providers, model names, and file paths.

## Setup

### Prerequisites

* Python 3.x
* A Google Gemini or OpenAI API key

### Clone the Repository

```bash
git clone https://github.com/jarrarte/langchain-interviewer.git
```

### Install Dependencies

The following dependencies are used:
* `langchain`
* `langchain-community`
* `langchain-google-genai`
* `langchain-openai`
* `python-dotenv`
* `faiss-cpu` (or `faiss-gpu` if you have CUDA set up)
* `pypdf`
* `reportlab`

You should create a virtual environment and download all the requirements using pip. 
Run:

```bash
cd langchain-interviewer
python -m venv venv
source venv/bin/activate
...
pip install -r requirements.txt
```

### API Keys

* Create a `.env` file in the project root directory.
* Add your API key(s) to the `.env` file. You need at least one (depending on the `llm_provider` in config.json):

```dotenv
GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY"
# or
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
```
The application will raise an error if the required key for the selected provider is not found.

### Configuration File (`config.json`)

Modify the `config.json` file in the project root. Specify the LLM provider, models, and file paths. 

Example:
```json
{
    "llm_provider": "google",

    "google_chat_model": "gemini-2.5-pro-exp-03-25",
    "google_embedding_model": "models/embedding-001",

    "openai_chat_model": "gpt-4o-mini",
    "openai_embedding_model": "text-embedding-3-small",

    "resume_path": "path/to/your/resume.pdf",
    "job_description_path": "path/to/your/job_description.pdf"
}
```
Valid options for `llm_provider` are currently `google` or `openai`.

If `config.json` is missing or invalid, defaults will be used. Default LLM provider is `google`.

### Input Files
Place the candidate's resume PDF and the job description PDF at the paths specified in `config.json`. 

### LangSmith Tracing (Optional)

[LangSmith](https://smith.langchain.com/) can be used for tracing, monitoring, and debugging LangChain applications. 

To enable LangSmith tracing, follow these steps:

#### Enable Tracing

Add the following to your `.env` file:
```dotenv
LANGCHAIN_TRACING_V2="true"
```

#### API Key

If tracing is enabled, you must provide your LangSmith API key:
```dotenv
LANGCHAIN_API_KEY="YOUR_LANGSMITH_API_KEY"
```

#### Optional Configuration

If you want to organize this project's calls in a specific **Project Name**, set `LANGCHAIN_PROJECT`:
```dotenv
LANGCHAIN_PROJECT="Technical-Interviewer-App"
```
If not set, runs will be assigned to the `default` project.

You can also specify LangSmith **API Endpoint**, but the default `https://api.smith.langchain.com` 
should be fine:

```dotenv
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
```

#### Usage
When enabled, LangSmith will automatically trace and log your application's interactions with LangChain, providing insights into its behavior and performance.

*Note:* Ensure your API key and endpoint are kept secure and not shared publicly.

## Application usage

1.  Run the main application script from the terminal:
    ```bash
    python interviewer.py
    ```
2.  The application will initialize, load the configured files and models, and start the interview simulation in the console.
3.  Interact with the interviewer by typing your answers when prompted.
4.  You can use commands like `stop`, `end interview`, or `finish` to end the session.
5.  You can guide the interview by saying things like `talk about AI` or `coding challenge`.

## Code Structure

* **`config.json`:** Stores configuration for LLM provider, models, and file paths.
* **Main Python Script (`*.py`):**
    * Imports necessary libraries (LangChain, providers, dotenv, etc.).
    * Loads environment variables (`.env`).
    * Defines Pydantic models (`WorkExperience`, `ResumeData`) for structured data extraction.
    * `TechnicalInterviewerApp` class:
        * `__init__`: Initializes LLM, embeddings, memory, loads documents, processes resume, creates vector store, defines chains.
        * `_load_and_process_resume`: Handles PDF loading and structured data extraction using LLM.
        * `_load_job_description`: Loads JD PDF.
        * `_initialize_vectorstore`: Creates FAISS vector store.
        * `_get_system_message`, `_get_human_instructions_for_stage`: Dynamically create prompts based on interview state.
        * `_generate_question`: Invokes the LLM chain to get the next question.
        * `_evaluate_answer`: Invokes the feedback chain.
        * `_transition_state`: Manages the flow between interview stages based on logic or user input.
        * `start_interview`: Main interactive loop.
    * `if __name__ == "__main__":`: Entry point, loads config, initializes and runs the `TechnicalInterviewerApp`.

## License

Free to use, just a sample project I created for learning how to use LangChain.

## Contributing

(TODO)