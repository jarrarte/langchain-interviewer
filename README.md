# LangChain Interviewer

## Overview

This is just a sample project I created for learning how to use LangChain.

This project is a Python application built using LangChain that simulates a technical interviewer. It aims to assess candidates by leveraging Large Language Models (LLMs) to process resumes, ask relevant questions based on interview stages, evaluate answers, and manage the overall interview flow. It supports multiple LLM providers like Google and OpenAI.

## Features

* **Multi-Stage Interview Simulation:** Guides the conversation through different stages: Icebreaker, Recent Experience, AI Basics, AI Advanced, Coding Setup, Coding Challenge, and Conclusion.
* **Resume Processing:** Loads candidate resumes from PDF files, extracts text, and uses an LLM to identify the candidate's name and structured work experience.
* **Job Description Loading:** Reads job descriptions from PDF files to potentially tailor the interview. (Note: Structured extraction from JD is commented out in the provided code).
* **Vector Store Integration:** Uses FAISS to create a vector store from the resume (and potentially job description) for retrieving relevant context during the interview.
* **LLM Provider Flexibility:** Supports Google (Gemini) and OpenAI (GPT) models, configurable via a JSON file.
* **Conversation Memory:** Maintains conversation history using `ConversationBufferMemory` to ensure contextually relevant follow-up questions.
* **Dynamic Question Generation:** Generates interview questions dynamically based on the current stage, conversation history, and extracted resume details.
* **Answer Evaluation:** Provides constructive feedback on the candidate's answers using a separate LLM chain.
* **Coding Challenge Module:** Allows setting preferred programming language (Python/Java) and difficulty (easy/medium/hard) for a coding problem generated by the LLM.
* **Configuration File:** Uses `config.json` for easy setup of LLM providers, model names, and file paths.

## Setup

1.  **Prerequisites:**
    * Python 3.x

2.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/jarrarte/langchain-interviewer.git](https://github.com/jarrarte/langchain-interviewer.git)
    cd langchain-interviewer
    ```

3.  **Install Dependencies:**
    The following dependencies are used:
    * `langchain`
    * `langchain-community`
    * `langchain-google-genai`
    * `langchain-openai`
    * `python-dotenv`
    * `faiss-cpu` (or `faiss-gpu` if you have CUDA set up)
    * `pypdf`
    * `reportlab`

    You can typically install them using pip. Create a `requirements.txt` file with the list above and run:
    ```bash
    pip install -r requirements.txt
    ```
    *Note:* If using OpenAI, ensure `langchain-openai` is installed. The script includes a check and provides an install command if it's missing.

4.  **API Keys:**
    * Create a `.env` file in the project root directory.
    * Add your API key(s) to the `.env` file. You need at least one:
        ```dotenv
        GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY"
        # or
        OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
        ```
        The application will raise an error if the required key for the selected provider is not found.

5.  **Configuration File (`config.json`):**
    * Create a `config.json` file in the project root or modify the existing one.
    * Specify the LLM provider, models, and file paths. Example:
        ```json
        {
          "llm_provider": "google",
          "google_chat_model": "gemini-1.5-flash-latest",
          "google_embedding_model": "models/embedding-001",
          "openai_chat_model": "gpt-4o-mini",
          "openai_embedding_model": "text-embedding-3-small",
          "resume_path": "path/to/your/resume.pdf",
          "job_description_path": "path/to/your/job_description.pdf"
        }
        ```
    * If `config.json` is missing or invalid, defaults will be used. Default provider is 'google'.

6.  **Input Files:**
    * Place the candidate's resume PDF and the job description PDF at the paths specified in `config.json`. Placeholder files might be included in the repository.

## Usage

1.  Run the main application script from the terminal:
    ```bash
    python your_main_script_name.py # Assuming the main script file name
    ```
2.  The application will initialize, load the configured files and models, and start the interview simulation in the console.
3.  Interact with the interviewer by typing your answers when prompted.
4.  You can use commands like `stop`, `end interview`, or `finish` to end the session.
5.  You can guide the interview by saying things like `talk about AI` or `coding challenge`.

## Code Structure (Brief Overview based on provided text)

* **`config.json`:** Stores configuration for LLM provider, models, and file paths.
* **Main Python Script (`*.py`):**
    * Imports necessary libraries (LangChain, providers, dotenv, etc.).
    * Loads environment variables (`.env`).
    * Defines Pydantic models (`WorkExperience`, `ResumeData`) for structured data extraction.
    * `TechnicalInterviewerApp` class:
        * `__init__`: Initializes LLM, embeddings, memory, loads documents, processes resume, creates vector store, defines chains.
        * `_load_and_process_resume`: Handles PDF loading and structured data extraction using LLM.
        * `_load_job_description`: Loads JD PDF.
        * `_initialize_vectorstore`: Creates FAISS vector store.
        * `_get_system_message`, `_get_human_instructions_for_stage`: Dynamically create prompts based on interview state.
        * `_generate_question`: Invokes the LLM chain to get the next question.
        * `_evaluate_answer`: Invokes the feedback chain.
        * `_transition_state`: Manages the flow between interview stages based on logic or user input.
        * `start_interview`: Main interactive loop.
    * `if __name__ == "__main__":`: Entry point, loads config, initializes and runs the `TechnicalInterviewerApp`.

## License

Free to use, just a sample project I created for learning how to use LangChain.

## Contributing

(TODO)